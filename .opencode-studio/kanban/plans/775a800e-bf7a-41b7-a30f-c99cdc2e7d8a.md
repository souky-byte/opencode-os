# API Integration Test: Full OpenCode Integration

## Task Summary
**Task ID:** 775a800e-bf7a-41b7-a30f-c99cdc2e7d8a
**Title:** API Integration Test
**Description:** Test full OpenCode integration
**Updated:** 2025-12-31

---

## 1. Technical Analysis

### Current State Assessment

The codebase has migrated to a single OpenCode integration approach:

**OpenAPI-Generated SDK** (`crates/opencode-client/`)
- Generated from OpenCode OpenAPI specification
- Used exclusively by `TaskExecutor` in orchestrator crate  
- Type-safe with comprehensive coverage of OpenCode API
- No unit tests (generated code, tested via integration)

### Existing Test Coverage

| Component | Tests | Coverage |
|-----------|-------|----------|
| Unit Tests (workspace-wide) | 108+ tests | Excellent |
| OpenCode Integration | 0 integration tests | None |
| SSE Event Streaming | 8 event tests | Good |
| Full Task Lifecycle | 0 end-to-end tests | None |
| API Endpoints | 12 server tests | Good (mocked) |

### Key Integration Points

```
HTTP API (/api/tasks/{id}/execute)
    ↓
TaskExecutor (crates/orchestrator/src/executor.rs)
    ↓
opencode-client SDK (crates/opencode-client/)
    ↓
OpenCode HTTP API (OPENCODE_URL, default: localhost:4096)
    ↓
SSE Events (EventBus -> /api/events, /api/sessions/{id}/activity)
    ↓
File Management (.opencode-studio/kanban/plans|reviews)
    ↓
VCS Workspace Management (Jujutsu/Git)
```

### Critical Gaps Identified

1. **No end-to-end API integration tests** - All tests use mocked OpenCode responses
2. **Full task lifecycle untested** - Complete TODO→DONE flow not verified
3. **Concurrent session handling untested** - Multiple OpenCode sessions not tested
4. **Error recovery scenarios untested** - OpenCode failures, network issues, timeouts
5. **VCS workspace integration untested** - Workspace creation/cleanup during OpenCode operations
6. **File persistence untested during failures** - Plan/review file handling edge cases
7. **SSE event emission verification missing** - Real-time events during OpenCode operations
8. **Performance characteristics unknown** - No load or timing tests

---

## 2. Files to Modify/Create

### New Integration Test Files

**Primary Test Suite:**
```
crates/server/tests/
├── api_integration_test.rs          # Main integration test suite
├── common/
│   ├── mod.rs                       # Shared test infrastructure
│   ├── opencode_mock.rs            # OpenCode server mock/simulator
│   ├── test_fixtures.rs            # Test data and factories
│   ├── assertions.rs               # Custom assertion helpers
│   └── database_utils.rs           # Test database utilities
```

**Specialized Test Files:**
```
crates/orchestrator/tests/
├── full_lifecycle_test.rs          # Complete task lifecycle tests
└── concurrent_execution_test.rs    # Concurrent OpenCode session tests

crates/server/tests/
├── sse_integration_test.rs         # Real-time event verification tests
└── performance_test.rs             # Load and timing tests
```

### Files to Modify

| File | Changes Required |
|------|------------------|
| `crates/server/Cargo.toml` | Add integration test dependencies (`wiremock`, `tokio-test`, `tempfile`, `uuid`) |
| `crates/orchestrator/Cargo.toml` | Add test dependencies for async testing |
| `crates/db/src/pool.rs` | Add test database utilities for isolated testing |
| `crates/server/src/lib.rs` | Export test configuration utilities |
| `.github/workflows/ci.yml` | Add integration test job (optional) |

### Enhanced Monitoring Files

| File | Purpose |
|------|---------|
| `crates/server/tests/test_config.rs` | Test environment configuration |
| `docs/testing/integration_testing.md` | Integration testing documentation |

---

## 3. Step-by-Step Implementation

### Phase 1: Test Infrastructure Setup (2 days)

**Step 1.1: Test Dependencies and Environment**
```rust
// crates/server/tests/common/mod.rs
/// Integration test utilities requiring live OpenCode server
/// Run with: OPENCODE_URL=http://localhost:4096 cargo test --package server --test api_integration_test

use std::env;
use opencode_client::apis::configuration::Configuration;

pub fn get_opencode_config() -> Option<Configuration> {
    env::var("OPENCODE_URL").ok().map(|url| {
        Configuration {
            base_path: url,
            ..Default::default()
        }
    })
}

pub fn skip_if_no_opencode() -> bool {
    env::var("OPENCODE_URL").is_err()
}
```

**Step 1.2: OpenCode Mock Server Infrastructure**
```rust
// crates/server/tests/common/opencode_mock.rs
use wiremock::{MockServer, Mock, ResponseTemplate};
use opencode_client::models::{SessionCreateRequest, SessionPromptRequest};

pub struct OpenCodeMock {
    pub server: MockServer,
    pub base_url: String,
}

impl OpenCodeMock {
    pub async fn start() -> Self { /* Implementation */ }
    pub fn mock_session_create(&self) -> Mock { /* Implementation */ }
    pub fn mock_session_prompt(&self) -> Mock { /* Implementation */ }
    pub fn simulate_delay(&self, ms: u64) { /* Implementation */ }
    pub fn simulate_error(&self, status: u16) { /* Implementation */ }
}
```

**Step 1.3: Test Database and File System Setup**
```rust
// crates/server/tests/common/test_fixtures.rs
use tempfile::TempDir;
use uuid::Uuid;

pub struct TestEnvironment {
    pub temp_dir: TempDir,
    pub db_url: String,
    pub task_id: Uuid,
    pub opencode_config: Configuration,
}

impl TestEnvironment {
    pub async fn setup() -> Self { /* Implementation */ }
    pub async fn cleanup(&self) { /* Implementation */ }
    pub fn create_test_task(&self) -> Task { /* Implementation */ }
}
```

**Step 1.4: SSE Event Testing Infrastructure**
```rust
// crates/server/tests/common/sse_test_client.rs
use futures::stream::StreamExt;

pub struct SSETestClient {
    client: reqwest::Client,
    base_url: String,
}

impl SSETestClient {
    pub async fn connect(&self, endpoint: &str) -> EventStream { /* Implementation */ }
    pub async fn collect_events_for(&self, duration: Duration) -> Vec<Event> { /* Implementation */ }
}
```

### Phase 2: API Endpoint Integration Tests (3 days)

**Step 2.1: Task CRUD with Real Database**
```rust
// Test: POST /api/tasks creates task in database
#[tokio::test]
async fn test_create_task_integration() {
    let env = TestEnvironment::setup().await;
    let response = test_client
        .post("/api/tasks")
        .json(&CreateTaskRequest { 
            title: "Integration Test Task".to_string(),
            description: "Test full OpenCode integration".to_string() 
        })
        .send().await?;
    
    assert_eq!(response.status(), 201);
    let task: Task = response.json().await?;
    assert_eq!(task.status, TaskStatus::Todo);
    
    // Verify database persistence
    let db_task = env.task_repository.find_by_id(task.id).await?.unwrap();
    assert_eq!(db_task.title, "Integration Test Task");
}
```

**Step 2.2: Task State Transitions**
- Test: `POST /api/tasks/{id}/transition` with valid state changes
- Test: Invalid transitions return 400 Bad Request
- Test: Task status persisted to database
- Test: Events emitted via SSE stream

**Step 2.3: Task Execution Endpoint - Basic Tests**
- Test: `POST /api/tasks/{id}/execute` with mocked OpenCode
- Test: Response format matches `ExecuteResponse` schema
- Test: Task status updated after execution
- Test: Session persistence to database

### Phase 3: Full Task Lifecycle Integration Tests (4 days)

**Step 3.1: Live OpenCode Planning Phase**
```rust
#[tokio::test] 
async fn test_planning_phase_with_live_opencode() {
    if skip_if_no_opencode() { return; }
    
    let env = TestEnvironment::setup().await;
    let task = env.create_test_task();
    
    // Execute planning phase
    let response = test_client
        .post(&format!("/api/tasks/{}/execute", task.id))
        .send().await?;
        
    let result: ExecuteResponse = response.json().await?;
    
    // Verify planning completion
    assert!(matches!(result.result, PhaseResultDto::PlanCreated { .. }));
    assert_eq!(result.task.status, TaskStatus::PlanningReview);
    
    // Verify plan file created
    let plan_path = format!(".opencode-studio/kanban/plans/{}.md", task.id);
    assert!(tokio::fs::metadata(&plan_path).await.is_ok());
    
    // Verify session persisted
    let sessions = env.session_repository.find_by_task_id(task.id).await?;
    assert_eq!(sessions.len(), 1);
    assert_eq!(sessions[0].phase, SessionPhase::Planning);
}
```

**Step 3.2: Implementation Phase with VCS Workspace**
- Test: VCS workspace created for implementation
- Test: Plan file loaded and included in implementation prompt
- Test: OpenCode tool calls executed in workspace context
- Test: Activity messages captured and stored
- Test: Workspace diff available after implementation

**Step 3.3: AI Review Phase**
- Test: Diff retrieved from VCS workspace
- Test: Review prompt includes actual code changes
- Test: "APPROVED" response format recognized correctly
- Test: "CHANGES_REQUESTED" triggers fix iteration
- Test: Max review iterations enforced (default 3)

**Step 3.4: Complete Task Lifecycle**
```rust
#[tokio::test]
async fn test_complete_todo_to_done_lifecycle() {
    if skip_if_no_opencode() { return; }
    
    let env = TestEnvironment::setup().await;
    let task = env.create_test_task();
    
    // Execute full cycle with relaxed approval settings
    let executor_config = ExecutorConfig::new(&env.temp_dir.path())
        .with_plan_approval(false)  // Auto-approve for testing
        .with_human_review(false);  // Auto-approve for testing
    
    let result = env.task_executor
        .run_full_cycle(&mut task).await?;
        
    assert!(matches!(result, PhaseResult::Completed));
    assert_eq!(task.status, TaskStatus::Done);
    
    // Verify all phases executed
    let sessions = env.session_repository.find_by_task_id(task.id).await?;
    assert!(sessions.len() >= 2); // At least planning + implementation
}
```

### Phase 4: Error Handling and Edge Cases (3 days)

**Step 4.1: OpenCode Server Failure Scenarios**
```rust
#[tokio::test]
async fn test_opencode_server_unavailable() {
    let env = TestEnvironment::setup().await;
    let task = env.create_test_task();
    
    // Configure with invalid OpenCode URL
    env::set_var("OPENCODE_URL", "http://localhost:9999");
    
    let response = test_client
        .post(&format!("/api/tasks/{}/execute", task.id))
        .send().await?;
        
    // Should return 500 Internal Server Error
    assert_eq!(response.status(), 500);
    
    // Task status should remain unchanged
    let updated_task = env.task_repository.find_by_id(task.id).await?.unwrap();
    assert_eq!(updated_task.status, TaskStatus::Todo);
}
```

**Step 4.2: Database Consistency During Failures**
- Test: Session persistence rolled back on OpenCode failure
- Test: Task status remains consistent after network errors
- Test: Activity store cleanup on session failure
- Test: File system cleanup on workspace errors

**Step 4.3: Concurrent Session Management**
```rust
#[tokio::test] 
async fn test_concurrent_task_execution() {
    if skip_if_no_opencode() { return; }
    
    let env = TestEnvironment::setup().await;
    let tasks = (0..3).map(|_| env.create_test_task()).collect::<Vec<_>>();
    
    // Execute all tasks concurrently
    let futures = tasks.iter().map(|task| {
        test_client.post(&format!("/api/tasks/{}/execute", task.id)).send()
    });
    
    let results = futures::future::join_all(futures).await;
    
    // All should succeed
    for result in results {
        assert_eq!(result?.status(), 200);
    }
    
    // Verify isolated execution (no state pollution)
    for task in &tasks {
        let updated_task = env.task_repository.find_by_id(task.id).await?.unwrap();
        assert!(updated_task.status != TaskStatus::Todo);
    }
}
```

**Step 4.4: Resource Cleanup and Limits**
- Test: VCS workspace cleanup after session completion
- Test: Plan/review file cleanup policies
- Test: Activity store memory limits during long sessions
- Test: OpenCode session cleanup on abort

### Phase 5: SSE Event Verification and Real-time Updates (2 days)

**Step 5.1: Global Event Stream Testing**
```rust
#[tokio::test]
async fn test_global_event_stream_during_execution() {
    if skip_if_no_opencode() { return; }
    
    let env = TestEnvironment::setup().await;
    let task = env.create_test_task();
    
    // Connect to global events stream
    let sse_client = SSETestClient::new(&env.server_url);
    let mut event_stream = sse_client.connect("/api/events").await?;
    
    // Trigger task execution
    let _response = test_client
        .post(&format!("/api/tasks/{}/execute", task.id))
        .send().await?;
    
    // Collect events for 30 seconds
    let events = sse_client.collect_events_for(Duration::from_secs(30)).await;
    
    // Verify expected event sequence
    assert!(events.iter().any(|e| matches!(e, Event::SessionStarted { task_id, .. } if *task_id == task.id)));
    assert!(events.iter().any(|e| matches!(e, Event::TaskStatusChanged { task_id, .. } if *task_id == task.id)));
    assert!(events.iter().any(|e| matches!(e, Event::SessionEnded { task_id, .. } if *task_id == task.id)));
}
```

**Step 5.2: Session-Specific Activity Stream**
- Test: `/api/sessions/{id}/activity` returns live OpenCode activity
- Test: Activity history + real-time streaming pattern
- Test: Reconnection delivers missed events
- Test: Activity parsing from OpenCode message parts

**Step 5.3: Event Buffer and Reconnection**
```rust
#[tokio::test]
async fn test_activity_reconnection_with_history() {
    if skip_if_no_opencode() { return; }
    
    let env = TestEnvironment::setup().await;
    let task = env.create_test_task();
    
    // Start execution to generate activity
    let execute_future = test_client.post(&format!("/api/tasks/{}/execute", task.id)).send();
    
    // Wait for session to start, then connect to activity stream
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    let session = env.session_repository.find_by_task_id(task.id).await?.first().unwrap().clone();
    let activity_stream = sse_client.connect(&format!("/api/sessions/{}/activity", session.id)).await?;
    
    // Should receive history + live updates
    let activities = sse_client.collect_events_for(Duration::from_secs(20)).await;
    assert!(!activities.is_empty());
    
    execute_future.await?;
}
```

### Phase 6: Performance and Documentation (2 days)

**Step 6.1: Performance Benchmarks**
- Test: API response times under normal load
- Test: Memory usage during multiple concurrent sessions  
- Test: Database query performance during complex operations
- Test: File system I/O performance for plan/review operations

**Step 6.2: Load Testing**
```rust
#[tokio::test]
async fn test_sustained_load_performance() {
    if skip_if_no_opencode() { return; }
    
    let env = TestEnvironment::setup().await;
    let start_time = Instant::now();
    
    // Create 10 tasks and execute them sequentially
    for i in 0..10 {
        let task = env.create_test_task();
        let response = test_client
            .post(&format!("/api/tasks/{}/execute", task.id))
            .send().await?;
        assert_eq!(response.status(), 200);
    }
    
    let total_time = start_time.elapsed();
    println!("10 sequential executions took: {:?}", total_time);
    assert!(total_time < Duration::from_secs(300)); // 5 minute limit
}
```

**Step 6.3: Test Documentation and CI Integration**
- Create `docs/testing/integration_testing.md` with setup instructions
- Add integration test job to `.github/workflows/ci.yml` (optional)
- Document test environment variables and dependencies
- Add troubleshooting guide for common test failures

---

## 4. Potential Risks

### High Risk

| Risk | Impact | Mitigation |
|------|--------|------------|
| **OpenCode server dependency for tests** | Tests cannot run without external service | Use comprehensive mocking with wiremock, implement optional integration mode with `OPENCODE_URL` env var |
| **Non-deterministic AI responses** | Flaky test failures due to varying OpenCode outputs | Assert on response structure/format rather than content, use controlled prompts, implement response validation patterns |
| **Long test execution times** | CI slowdown, developer workflow interruption | Implement test timeouts (5-30s per test), categorize fast vs. slow tests, use concurrent execution where safe |
| **Test data isolation failures** | Tests interfere with each other through shared state | Use isolated test databases per test case, implement thorough cleanup, use UUID-based test identifiers |

### Medium Risk

| Risk | Impact | Mitigation |
|------|--------|------------|
| **VCS workspace conflicts** | Tests fail due to workspace state pollution | Use temporary directories for each test, implement workspace cleanup, use isolated Git/Jujutsu repos |
| **Database transaction deadlocks** | Concurrent tests fail with database errors | Use test database connection pooling, implement retry logic, serialize database-heavy tests |
| **SSE connection management** | Event streams leak or fail to close properly | Implement connection timeouts, proper stream cleanup, connection pooling limits |
| **File system race conditions** | Plan/review file operations conflict | Use file locking, atomic write operations, unique file names per test |

### Low Risk

| Risk | Impact | Mitigation |
|------|--------|------------|
| **CI resource exhaustion** | Tests consume excessive CI resources | Monitor resource usage, implement resource limits, optimize test efficiency |
| **OpenCode API rate limiting** | Tests throttled during rapid execution | Add delays between OpenCode requests, implement request queuing |
| **Environment variable conflicts** | Test configuration interferes with development | Use test-specific environment variable prefixes, document configuration clearly |

---

## 5. Estimated Complexity

**Overall: XL (Extra Large)**

| Phase | Complexity | Days | Rationale |
|-------|------------|------|-----------|
| Test Infrastructure Setup | L | 2 | Complex async test framework, OpenCode mocking, database setup |
| API Endpoint Integration | M | 3 | HTTP integration testing, database verification |
| Full Task Lifecycle | XL | 4 | Multi-phase OpenCode integration, VCS workspace management, file operations |
| Error Handling & Edge Cases | L | 3 | Network failures, concurrent scenarios, resource cleanup |  
| SSE Events & Real-time | M | 2 | Event streaming, timing-sensitive tests |
| Performance & Documentation | S | 2 | Benchmarking, documentation |

**Detailed Effort Breakdown:**
- **Test Infrastructure (2 days)**: OpenCode mock server, test database utilities, SSE test client, file system mocking
- **API Integration (3 days)**: CRUD operations, state transitions, basic execution endpoint testing
- **Lifecycle Testing (4 days)**: Planning → Implementation → Review → Done flow, VCS integration, file persistence 
- **Error Scenarios (3 days)**: Network failures, database consistency, concurrent execution, resource limits
- **Real-time Events (2 days)**: SSE stream verification, activity streaming, reconnection logic
- **Performance & Docs (2 days)**: Load testing, benchmarks, documentation, CI integration

**Total Estimated Effort: 16 days**

### Complexity Factors

**High Complexity Elements:**
- Async coordination between API, database, OpenCode, VCS, and file system
- OpenCode response parsing and activity message handling
- Concurrent session management and isolation
- State consistency verification across distributed operations
- SSE event timing and sequence verification

**Medium Complexity Elements:**
- Test database setup and migration management  
- HTTP integration testing patterns
- File system operations testing
- Error simulation and recovery testing

**Low Complexity Elements:**
- Basic API endpoint testing
- Simple state transition verification
- Test fixture creation and cleanup
- Documentation writing

---

## 6. Test Strategy

### Test Pyramid Enhancement

```
                     /\
                    /  \
                   / E2E \ (10-15 integration tests)
                  /      \
                 /--------\
                / Integration \ (35-50 tests total)  
               /              \
              /----------------\
             /     Unit Tests    \ (existing 108+ tests)
            /____________________\
```

**Test Categories:**
- **Unit Tests (108+)**: Existing crate-level tests covering individual components
- **Integration Tests (35-50)**: New API integration tests with live/mocked OpenCode
- **End-to-End (10-15)**: Complete task lifecycle tests with full system integration

### Test Execution Modes

**Mode 1: Mock-Only Integration Tests (Default)**
```bash
# Fast execution, no external dependencies
cargo test --package server --test api_integration_test
```

**Mode 2: Live OpenCode Integration Tests**
```bash
# Requires running OpenCode server
OPENCODE_URL=http://localhost:4096 cargo test --package server --test api_integration_test
```

**Mode 3: Performance and Load Tests**
```bash
# Long-running performance tests
OPENCODE_URL=http://localhost:4096 cargo test --package server --test performance_test --release
```

### Environment Configuration

```toml
# crates/server/Cargo.toml
[dev-dependencies]
wiremock = "0.6"
tokio-test = "0.4"  
tempfile = "3.10"
uuid = { version = "1.6", features = ["v4"] }
futures = "0.3"

[[test]]
name = "api_integration_test"
path = "tests/api_integration_test.rs"

[[test]] 
name = "performance_test"
path = "tests/performance_test.rs"
```

### Test Data Management

```rust
// Test isolation strategy
pub struct TestEnvironment {
    pub temp_dir: TempDir,           // Isolated file system
    pub db_url: String,              // Unique test database  
    pub opencode_config: Configuration,
    pub test_id: Uuid,               // Test run identifier
}

impl Drop for TestEnvironment {
    fn drop(&mut self) {
        // Automatic cleanup of all test resources
    }
}
```

---

## 7. Success Criteria

### Must Have (Required for Completion)
- [ ] **Full task lifecycle tested end-to-end** - Complete TODO→DONE automation verified
- [ ] **All OpenCode integration points covered** - Planning, implementation, review phases tested
- [ ] **Error handling scenarios verified** - OpenCode failures, network issues, timeouts handled gracefully
- [ ] **Database consistency maintained** - All operations maintain data integrity during failures
- [ ] **SSE event emission verified** - Real-time events properly emitted during task execution
- [ ] **VCS workspace integration tested** - Workspace creation, diff retrieval, cleanup working correctly
- [ ] **Concurrent session support** - Multiple OpenCode sessions can run simultaneously without interference

### Should Have (Highly Desirable)
- [ ] **Performance benchmarks established** - Response time and resource usage metrics documented
- [ ] **Test execution time optimized** - Full integration test suite completes in under 10 minutes
- [ ] **Comprehensive error scenarios** - Network failures, database errors, file system issues covered
- [ ] **Activity stream verification** - Real-time activity updates tested end-to-end
- [ ] **Resource cleanup verified** - No resource leaks or orphaned processes after test completion

### Nice to Have (Optional Enhancement)
- [ ] **CI integration configured** - Tests run automatically in CI environment  
- [ ] **Load testing implemented** - System behavior under sustained load documented
- [ ] **Test documentation comprehensive** - Setup, troubleshooting, and maintenance guides available
- [ ] **Mock vs. live test modes** - Tests can run with both mocked and real OpenCode server

---

## 8. Dependencies

### Critical Dependencies
- **OpenCode Server**: Live OpenCode instance required for integration testing
- **Database**: SQLite database with migration capabilities
- **VCS**: Git or Jujutsu for workspace management testing
- **File System**: Write permissions for plan/review file operations

### Development Dependencies
- **Rust Test Framework**: tokio-test, wiremock for async testing
- **Temporary Resources**: tempfile for isolated test environments  
- **HTTP Client**: reqwest for API testing
- **Event Streaming**: SSE client capabilities for real-time verification

### Optional Dependencies
- **Docker**: For containerized OpenCode testing environment
- **CI Environment**: GitHub Actions or similar for automated testing
- **Monitoring Tools**: Performance measurement and resource tracking

---

## 9. Implementation Notes

### Current Codebase Integration

**Generated OpenCode Client Usage:**
```rust
// Current implementation in crates/orchestrator/src/executor.rs
use opencode_client::apis::configuration::Configuration;
use opencode_client::apis::default_api;
use opencode_client::models::{SessionCreateRequest, SessionPromptRequest};
```

**Key Architectural Components:**
- `TaskExecutor` in `crates/orchestrator/src/executor.rs` - Main OpenCode integration point
- `SessionActivityRegistry` - Real-time activity management
- `FileManager` - Plan/review file persistence
- `WorkspaceManager` - VCS integration for code changes

### Test Infrastructure Patterns

**Following Existing Patterns from Current Tests:**
1. **Isolated Test Databases**: Each test uses separate SQLite database
2. **Temporary Directories**: Use tempfile crate for file system isolation  
3. **Async Test Coordination**: tokio-test for async test execution
4. **Mock Server Integration**: wiremock for controlled OpenCode simulation

### Critical Testing Areas

**State Machine Verification:**
- Task status transitions follow defined state machine rules
- Invalid transitions properly rejected
- Database and memory state remain consistent

**Event System Testing:**
- EventBus properly emits events during operations
- SSE streams deliver events in correct order
- Event buffers handle reconnection scenarios

**Resource Management:**
- VCS workspaces created and cleaned up properly
- OpenCode sessions properly terminated
- Activity stores don't leak memory during long operations

---

**Next Steps:**
1. **Environment Setup**: Configure development environment with required dependencies
2. **Phase 1 Implementation**: Begin with test infrastructure setup (OpenCode mock, database utils)  
3. **Incremental Development**: Implement one test phase at a time with regular verification
4. **Stabilization**: Run tests repeatedly to identify and fix flaky behavior
5. **Documentation**: Document test setup, execution, and troubleshooting procedures
